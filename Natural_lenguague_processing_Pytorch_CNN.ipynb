{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WlFS_v7TpRHe"
      },
      "source": [
        "# Homework 3: Pytorch and CNNs\n",
        "\n",
        "In this homework, you will begin exploring Pytorch, a neural network library that will be used throughout the remainder of the semester.  This homework will focus on implementing a bag-of-words logistic regression and a convolutional neural networks.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wyY0yl1Jpf2o"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import nltk\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import nltk\n",
        "from collections import Counter\n",
        "\n",
        "#Sets random seeds for reproducibility\n",
        "seed=159259\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.backends.cudnn.deterministic = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s7DVjxeq_-OF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1f56308-86c9-43fc-8628-cb7078c65f6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/lib/python3.7/runpy.py:125: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
            "  warn(RuntimeWarning(msg))\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "!python -m nltk.downloader punkt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlYfHZwlQXA_"
      },
      "source": [
        "When looking up pytorch documentation, it may be useful to know which version of torch you are running.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qUdEHON5lybF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31457f84-64ce-4748-bed7-1f902dbfd858"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.10.0+cu111\n"
          ]
        }
      ],
      "source": [
        "print(torch.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xrbyc1flKp_"
      },
      "source": [
        "# **IMPORTANT**: GPU is not enabled by default\n",
        "\n",
        "You must switch runtime environments if your output of the next block of code has an error saying \"ValueError: Expected a cuda device, but got: cpu\"\n",
        "\n",
        "Go to Runtime > Change runtime type > Hardware accelerator > GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mRy4VWrvkCP6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc29b966-f95f-4b54-e8a9-c4b744cc2783"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on cuda\n"
          ]
        }
      ],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Running on {}\".format(device))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XyARzkPKmUlR"
      },
      "source": [
        "# Data Processing\n",
        "\n",
        "Let's begin by loading our datasets and the 50-dimensional GLoVE word embeddings.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l_ZZQsGwH5vj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "115682f2-f68f-43dc-ef73-6a8a27df57a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-02-09 00:24:20--  https://raw.githubusercontent.com/dbamman/nlp22/main/HW3/train.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6645801 (6.3M) [text/plain]\n",
            "Saving to: ‘train.txt.1’\n",
            "\n",
            "train.txt.1         100%[===================>]   6.34M  --.-KB/s    in 0.08s   \n",
            "\n",
            "2022-02-09 00:24:21 (80.3 MB/s) - ‘train.txt.1’ saved [6645801/6645801]\n",
            "\n",
            "--2022-02-09 00:24:21--  https://raw.githubusercontent.com/dbamman/nlp22/main/HW3/dev.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1309909 (1.2M) [text/plain]\n",
            "Saving to: ‘dev.txt.1’\n",
            "\n",
            "dev.txt.1           100%[===================>]   1.25M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2022-02-09 00:24:21 (23.2 MB/s) - ‘dev.txt.1’ saved [1309909/1309909]\n",
            "\n",
            "--2022-02-09 00:24:21--  https://raw.githubusercontent.com/dbamman/nlp22/main/HW3/glove.6B.50d.50K.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 21357789 (20M) [text/plain]\n",
            "Saving to: ‘glove.6B.50d.50K.txt.1’\n",
            "\n",
            "glove.6B.50d.50K.tx 100%[===================>]  20.37M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2022-02-09 00:24:21 (153 MB/s) - ‘glove.6B.50d.50K.txt.1’ saved [21357789/21357789]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/dbamman/nlp22/main/HW3/train.txt\n",
        "!wget https://raw.githubusercontent.com/dbamman/nlp22/main/HW3/dev.txt\n",
        "!wget https://raw.githubusercontent.com/dbamman/nlp22/main/HW3/glove.6B.50d.50K.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vC5tWWn2mWhH"
      },
      "outputs": [],
      "source": [
        "trainingFile = \"train.txt\"\n",
        "devFile = \"dev.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q_vLcPzzIxDw"
      },
      "outputs": [],
      "source": [
        "labels = {'pos': 0, 'neg': 1}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sNb4H1auI4lA"
      },
      "outputs": [],
      "source": [
        "def get_batches(x, y, xType, batch_size=12):\n",
        "    batches_x=[]\n",
        "    batches_y=[]\n",
        "    for i in range(0, len(x), batch_size):\n",
        "        batches_x.append(xType(x[i:i+batch_size]))\n",
        "        batches_y.append(torch.LongTensor(y[i:i+batch_size]))\n",
        "    \n",
        "    return batches_x, batches_y\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SnIbufFHlYSx"
      },
      "outputs": [],
      "source": [
        "PAD_INDEX = 0             # reserved for padding words\n",
        "UNKNOWN_INDEX = 1         # reserved for unknown words\n",
        "SEP_INDEX = 2\n",
        "\n",
        "data_lens = []\n",
        "\n",
        "def read_embeddings(filename, vocab_size=50000):\n",
        "  \"\"\"\n",
        "  Utility function, loads in the `vocab_size` most common embeddings from `filename`\n",
        "  \n",
        "  Arguments:\n",
        "  - filename:     path to file\n",
        "                  automatically infers correct embedding dimension from filename\n",
        "  - vocab_size:   maximum number of embeddings to load\n",
        "\n",
        "  Returns \n",
        "  - embeddings:   torch.FloatTensor matrix of size (vocab_size x word_embedding_dim)\n",
        "  - vocab:        dictionary mapping word (str) to index (int) in embedding matrix\n",
        "  \"\"\"\n",
        "\n",
        "  # get the embedding size from the first embedding\n",
        "  with open(filename, encoding=\"utf-8\") as file:\n",
        "    word_embedding_dim = len(file.readline().split(\" \")) - 1\n",
        "\n",
        "  vocab = {}\n",
        "\n",
        "  embeddings = np.zeros((vocab_size, word_embedding_dim))\n",
        "  with open(filename, encoding=\"utf-8\") as file:\n",
        "    for idx, line in enumerate(file):\n",
        "\n",
        "      if idx + 2 >= vocab_size:\n",
        "        break\n",
        "\n",
        "      cols = line.rstrip().split(\" \")\n",
        "      val = np.array(cols[1:])\n",
        "      word = cols[0]\n",
        "      embeddings[idx + 2] = val\n",
        "      vocab[word] = idx + 2\n",
        "  \n",
        "  # a FloatTensor is a multidimensional matrix\n",
        "  # that contains 32-bit floats in every entry\n",
        "  # https://pytorch.org/docs/stable/tensors.html\n",
        "  return torch.FloatTensor(embeddings), vocab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrBHMiLPIOKB"
      },
      "source": [
        "# Logistic regression\n",
        "\n",
        "First, let's code up logistic regression in pytorch so you can see how the general framework works, and also get a sense of its performance that we can compare a CNN against."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "392D8YLfI_K3"
      },
      "outputs": [],
      "source": [
        "class LogisticRegressionClassifier(nn.Module):\n",
        "\n",
        "   def __init__(self, input_dim, output_dim):\n",
        "      super().__init__()\n",
        "      self.linear = torch.nn.Linear(input_dim, output_dim)\n",
        " \n",
        "    \n",
        "   def forward(self, input): \n",
        "      x1 = self.linear(input)\n",
        "      return x1\n",
        "\n",
        "   def evaluate(self, x, y):\n",
        "\n",
        "      self.eval()\n",
        "      corr = 0.\n",
        "      total = 0.\n",
        "      with torch.no_grad():\n",
        "        for x, y in zip(x, y):\n",
        "          x, y = x.to(device), y.to(device)\n",
        "          y_preds=self.forward(x)\n",
        "          for idx, y_pred in enumerate(y_preds):\n",
        "              prediction=torch.argmax(y_pred)\n",
        "              if prediction == y[idx]:\n",
        "                corr += 1.\n",
        "              total+=1                          \n",
        "      return corr/total\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgaDKtOrc10l"
      },
      "source": [
        "## Example: Average Embedding Representation\n",
        "Let's train a logistic regression classifier where the input is the average GLoVE embedding for all words in a review."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0YgU4027luO3"
      },
      "outputs": [],
      "source": [
        "def read_glove_data(filename, vocab, embs):\n",
        "    data=[]\n",
        "    data_labels=[]\n",
        "    with open(filename) as file:\n",
        "        for line in file:\n",
        "            avg_emb=np.zeros(50)\n",
        "            cols=line.rstrip().split(\"\\t\")\n",
        "            idd = cols[0]\n",
        "            label = cols[1]\n",
        "            review = cols[2]\n",
        "            words=nltk.word_tokenize(review)\n",
        "            avg_counter = 0.\n",
        "            for word in words:\n",
        "                word=word.lower()\n",
        "                if word in glove_vocab:\n",
        "                    avg_emb += embs[glove_vocab[word]].numpy()\n",
        "                    avg_counter += 1.\n",
        "            avg_emb /= avg_counter\n",
        "\n",
        "            data.append(avg_emb)\n",
        "            data_labels.append(labels[label])\n",
        "    return data, data_labels \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JYb1iVsqb0Le"
      },
      "outputs": [],
      "source": [
        "embs, glove_vocab = read_embeddings(\"glove.6B.50d.50K.txt\")\n",
        "avg_train_x, avg_train_y=read_glove_data(trainingFile, glove_vocab, embs)\n",
        "avg_dev_x, avg_dev_y=read_glove_data(devFile, glove_vocab, embs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-FeYYf7-c01Z"
      },
      "outputs": [],
      "source": [
        "avg_trainX, avg_trainY=get_batches(avg_train_x, avg_train_y, xType=torch.FloatTensor)\n",
        "avg_devX, avg_devY=get_batches(avg_dev_x, avg_dev_y, xType=torch.FloatTensor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Duzn0vCrdR5X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc662647-1253-447b-c33b-67d30e267fef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, dev accuracy: 0.649\n",
            "Epoch 5, dev accuracy: 0.684\n",
            "Epoch 10, dev accuracy: 0.702\n",
            "Epoch 15, dev accuracy: 0.723\n",
            "Epoch 20, dev accuracy: 0.728\n",
            "Epoch 25, dev accuracy: 0.734\n",
            "Epoch 30, dev accuracy: 0.737\n",
            "Epoch 35, dev accuracy: 0.737\n",
            "Epoch 40, dev accuracy: 0.739\n",
            "Stopping training; no improvement on dev data after 10 epochs\n"
          ]
        }
      ],
      "source": [
        "logreg=LogisticRegressionClassifier(50, len(labels)).to(device)\n",
        "optimizer = torch.optim.Adam(logreg.parameters(), lr=0.001, weight_decay=1e-5)\n",
        "losses = []\n",
        "cross_entropy=nn.CrossEntropyLoss()\n",
        "\n",
        "num_labels=len(labels)\n",
        "\n",
        "patience=10\n",
        "maxDevAccuracy=0\n",
        "patienceCounter=0\n",
        "\n",
        "for epoch in range(200):\n",
        "    logreg.train()\n",
        "    \n",
        "    for x, y in zip(avg_trainX, avg_trainY):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        y_pred=logreg.forward(x)\n",
        "        loss = cross_entropy(y_pred.view(-1, num_labels), y.view(-1))\n",
        "        losses.append(loss)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    dev_accuracy=logreg.evaluate(avg_devX, avg_devY)\n",
        "    \n",
        "    # check if the dev accuracy is the best seen so far\n",
        "    if dev_accuracy > maxDevAccuracy:\n",
        "        maxDevAccuracy=dev_accuracy\n",
        "        patienceCounter=0\n",
        "    \n",
        "    patienceCounter+=1\n",
        "    \n",
        "    if epoch % 5 == 0:\n",
        "        print(\"Epoch %s, dev accuracy: %.3f\" % (epoch, dev_accuracy))\n",
        "    if patienceCounter >= patience:\n",
        "        print(\"Stopping training; no improvement on dev data after %s epochs\" % patience)\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObjO1BiXc_nY"
      },
      "source": [
        "# Deliverable 1. BOW Representation\n",
        "Your last homework used sklearn for logistic regression classification using a bag-of-words representation. Here you'll do the same thing, but in pytorch.  Fill in a bag-of-words implementation into read_bow_data() to see how the logistic classifier model works with this different featurization. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0r3LM0mKDxVP"
      },
      "outputs": [],
      "source": [
        "# This function creates a unigram vocabulary from the most frequent 10K words in the training data\n",
        "def get_vocab(filename, max_words=10000):\n",
        "    unigram_counts=Counter()\n",
        "    with open(filename) as file:    \n",
        "        for line in file:\n",
        "            cols=line.rstrip().split(\"\\t\")\n",
        "            idd = cols[0]\n",
        "            label = cols[1]\n",
        "            review = cols[2]\n",
        "            words=nltk.word_tokenize(review)\n",
        "\n",
        "            for word in words:\n",
        "                word=word.lower()\n",
        "                unigram_counts[word]+=1\n",
        "\n",
        "    vocab={}\n",
        "    for k,v in unigram_counts.most_common(max_words):\n",
        "        vocab[k]=len(vocab)\n",
        "    return vocab\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "huWSY2FNlqrF"
      },
      "outputs": [],
      "source": [
        "def read_bow_data(filename, vocab):\n",
        "    data=[]\n",
        "    data_labels=[]\n",
        "    with open(filename) as file:\n",
        "        for line in file:\n",
        "            cols=line.rstrip().split(\"\\t\")\n",
        "            idd = cols[0]\n",
        "            label = cols[1]\n",
        "            review = cols[2]\n",
        "            bow=np.zeros(len(vocab))\n",
        "\n",
        "            words=nltk.word_tokenize(review)\n",
        "\n",
        "            '''\n",
        "            Add your bow code here to store the featurization in the bow variable.\n",
        "            \n",
        "            \n",
        "            '''\n",
        "            #You should be iterating through all the words (tokens) in the review,\n",
        "            #So that you can create features that indicate the presence of a word.\n",
        "            for word in words:\n",
        "              word=word.lower()\n",
        "              index = vocab.get(word)\n",
        "              if isinstance(index, int):\n",
        "                bow[index] = 1\n",
        "\n",
        "            data.append(bow)\n",
        "\n",
        "            data_labels.append(labels[label])\n",
        "    return data, data_labels \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tc9ZGJvWImkA"
      },
      "outputs": [],
      "source": [
        "bow_vocab=get_vocab(trainingFile)\n",
        "bow_train_x, bow_train_y=read_bow_data(trainingFile, bow_vocab)\n",
        "bow_dev_x, bow_dev_y=read_bow_data(devFile, bow_vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LFucMsZII8Hb"
      },
      "outputs": [],
      "source": [
        "bow_trainX, bow_trainY=get_batches(bow_train_x, bow_train_y, xType=torch.FloatTensor)\n",
        "bow_devX, bow_devY=get_batches(bow_dev_x, bow_dev_y, xType=torch.FloatTensor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "byr4SJB1JCDO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6640c131-7bd3-4b82-ac9d-b77ea11e73fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, dev accuracy: 0.844\n",
            "Epoch 5, dev accuracy: 0.870\n",
            "Epoch 10, dev accuracy: 0.859\n",
            "Stopping training; no improvement on dev data after 10 epochs\n",
            "\n",
            "Best Performing Model achieves dev accuracy of : 0.870\n"
          ]
        }
      ],
      "source": [
        "logreg=LogisticRegressionClassifier(len(bow_vocab), len(labels)).to(device)\n",
        "optimizer = torch.optim.Adam(logreg.parameters(), lr=0.001, weight_decay=1e-5)\n",
        "losses = []\n",
        "cross_entropy=nn.CrossEntropyLoss()\n",
        "best_dev_acc = 0.\n",
        "\n",
        "num_labels=len(labels)\n",
        "\n",
        "patience=10\n",
        "patienceCounter=0\n",
        "\n",
        "for epoch in range(200):\n",
        "    for x, y in zip(bow_trainX, bow_trainY):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        y_pred=logreg.forward(x)\n",
        "        loss = cross_entropy(y_pred.view(-1, num_labels), y.view(-1))\n",
        "        losses.append(loss)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    dev_accuracy=logreg.evaluate(bow_devX, bow_devY)\n",
        "            \n",
        "    if epoch % 5 == 0:\n",
        "        print(\"Epoch %s, dev accuracy: %.3f\" % (epoch, dev_accuracy))\n",
        "\n",
        "    # check if the dev accuracy is the best seen so far; save the model if so\n",
        "    if dev_accuracy > best_dev_acc:\n",
        "      torch.save(logreg.state_dict(), 'best-bowmodel-parameters.pt')\n",
        "      best_dev_acc = dev_accuracy\n",
        "      patienceCounter=0\n",
        "\n",
        "    patienceCounter+=1\n",
        "    if patienceCounter >= patience:\n",
        "        print(\"Stopping training; no improvement on dev data after %s epochs\" % patience)\n",
        "        break\n",
        "    \n",
        "logreg.load_state_dict(torch.load('best-bowmodel-parameters.pt'))\n",
        "print(\"\\nBest Performing Model achieves dev accuracy of : %.3f\" % (best_dev_acc))    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbHrmE4jJQrT"
      },
      "source": [
        "# Deliverable 2. CNN \n",
        "\n",
        "Now let's create our CNN."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9YhST7BOJPoG"
      },
      "outputs": [],
      "source": [
        "def read_data(filename, vocab, labels):\n",
        "    \"\"\"\n",
        "    :param filename: the name of the file\n",
        "    :return: list of tuple ([word index list], label)\n",
        "    as input for the forward and backward function\n",
        "    \"\"\"    \n",
        "    data = []\n",
        "    data_labels = []\n",
        "    file = open(filename)\n",
        "    for line in file:\n",
        "        cols = line.split(\"\\t\")\n",
        "        idd = cols[0]\n",
        "        label = cols[1]\n",
        "        review = cols[2]\n",
        "        w_int = []\n",
        "        for w in nltk.word_tokenize(review.lower()):\n",
        "            if w in vocab:\n",
        "                w_int.append(vocab[w])\n",
        "            else:\n",
        "                w_int.append(UNKNOWN_INDEX)\n",
        "        data_lens.append(len(w_int))\n",
        "        if len(w_int) < 549:\n",
        "            w_int.extend([PAD_INDEX] * (549 - len(w_int)))\n",
        "        if len(w_int) < 550:\n",
        "          data.append((w_int))\n",
        "          data_labels.append(labels[label])\n",
        "    file.close()\n",
        "    return data, data_labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9sB60ratJZvB"
      },
      "outputs": [],
      "source": [
        "embs, cnn_vocab = read_embeddings(\"glove.6B.50d.50K.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hStl2tmiJesV"
      },
      "outputs": [],
      "source": [
        "cnn_train_x, cnn_train_y = read_data(trainingFile, cnn_vocab, labels)\n",
        "cnn_dev_x, cnn_dev_y = read_data(devFile, cnn_vocab, labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gZNvT-REJgv1"
      },
      "outputs": [],
      "source": [
        "cnn_trainX, cnn_trainY=get_batches(cnn_train_x, cnn_train_y, torch.LongTensor)\n",
        "cnn_devX, cnn_devY=get_batches(cnn_dev_x, cnn_dev_y, torch.LongTensor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LDGz8mqdJjic"
      },
      "outputs": [],
      "source": [
        "class CNNClassifier(nn.Module):\n",
        "## This model performs 3 ifferent convolutios on the input data, of window sizes 1, 2, 3.\n",
        "## The input data to this model is a sequence of tokens consiting of a movie's reiew\n",
        "## All examples are padded to a maximum length of 50-dimensional GloVe word embeddings\n",
        "\n",
        "   def __init__(self, params, pretrained_embeddings):\n",
        "     #Model's initiatlitaion method creates and initializes the layes with their correct sizes.\n",
        "      super().__init__()\n",
        "      self.seq_len = params[\"max_seq_len\"]\n",
        "      self.num_labels = params[\"label_length\"]\n",
        "      \n",
        "      '''\n",
        "      Initialize the following layers according to the hw spec\n",
        "      '''\n",
        "      #This shoudl initialize embeddings from the pretrained_embeddings parameter.\n",
        "      #These embeddings shouldbe initialized to the GLoVE embedding values bur should update during training.\n",
        "      self.embeddings = nn.Embedding(len(pretrained_embeddings),50)\n",
        "\n",
        "      # convolution over 1 word\n",
        "      # This layer should be a nn.Conv1d layer with 50 filters which convolves over the GLoVE embeddings.\n",
        "      # Self.conv_1 should have a 1-word window.\n",
        "      # Stride of 1.\n",
        "\n",
        "      self.conv_1 = nn.Conv1d(50, 50, 1, stride=1)\n",
        "\n",
        "      # convolution over 2 words\n",
        "      # This layer should be a nn.Conv1d layer with 50 filters which convolves over the GLoVE embeddings.\n",
        "      # Self.conv_1 should have a 2-word window.\n",
        "      # Stride of 1.    \n",
        "      self.conv_2 = nn.Conv1d(50, 50, 2, stride=1)\n",
        "        \n",
        "      # convolution over 3 words\n",
        "      # This layer should be a nn.Conv1d layer with 50 filters which convolves over the GLoVE embeddings.\n",
        "      # Self.conv_1 should have a 3-word window.\n",
        "      # Stride of 1.    \n",
        "\n",
        "      self.conv_3 = nn.Conv1d(50, 50, 3, stride=1)\n",
        "      \n",
        "      #This should be a linear layer which takes in the concatenated 1-,2-,and 3-2ord CNN representations\n",
        "      #And outputs a prediction over the output classes!\n",
        "      self.fc = nn.Linear(in_features = 50 * 3, out_features= self.num_labels)\n",
        "\n",
        "\n",
        "   def forward(self, input): \n",
        "      #embeds the input sequences\n",
        "      x0 = self.embeddings(input)\n",
        "      #changes dimensions to be consistent with conv1d\n",
        "      x0 = x0.permute(0, 2, 1)\n",
        "\n",
        "      '''\n",
        "      Create the hidden representations according to the hw spec\n",
        "      '''\n",
        "\n",
        "      # Apply the one-word convolution, tanh, and max pool\n",
        "      # x1: Should contain the representation learned from the 1-word convolutional layer\n",
        "      # the 1-word convolution over the GLoVE embeddings, a tanh activation function, with max pooling\n",
        "      x1 = self.conv_1(x0)\n",
        "      x1 = nn.functional.tanh(x1)\n",
        "      x1 = torch.max(x1, dim = 2)[0]\n",
        "\n",
        "      # Apply the two-word convolution, tanh, and max pool\n",
        "      x2 = self.conv_2(x0)\n",
        "      x2 = nn.functional.tanh(x2)\n",
        "      x2 = torch.max(x2,dim = 2 )[0]\n",
        "\n",
        "      # Apply the three-word convolution, tanh, and max pool\n",
        "      x3 = self.conv_3(x0)\n",
        "      x3 = nn.functional.tanh(x3)\n",
        "      x3 = torch.max(x3, dim = 2 )[0]\n",
        "\n",
        "      # Concatenates the output of all 3 convolution layers\n",
        "      combined = torch.cat( (x1,x2,x3) , dim = 1 )\n",
        "\n",
        "      # Connects the combined output to the fully-connected layer\n",
        "      out = self.fc(combined)\n",
        "      return out.squeeze()\n",
        "\n",
        "\n",
        "   def evaluate(self, x, y):\n",
        "      \n",
        "      self.eval()\n",
        "      corr = 0.\n",
        "      total = 0.\n",
        "\n",
        "      with torch.no_grad():\n",
        "\n",
        "        for x, y in zip(x, y):\n",
        "          x, y = x.to(device), y.to(device)\n",
        "          y_preds=self.forward(x)\n",
        "          for idx, y_pred in enumerate(y_preds):\n",
        "              prediction=torch.argmax(y_pred)\n",
        "              if prediction == y[idx]:\n",
        "                corr += 1.\n",
        "              total+=1                          \n",
        "      return corr/total\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DxrBo0N0JlGM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fb24ec6-a2f4-4349-dece-e0cecfd2fbbc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1795: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, dev accuracy: 0.695\n",
            "Epoch 1, dev accuracy: 0.755\n",
            "Epoch 2, dev accuracy: 0.779\n",
            "Epoch 3, dev accuracy: 0.785\n",
            "Epoch 4, dev accuracy: 0.798\n",
            "Epoch 5, dev accuracy: 0.797\n",
            "Epoch 6, dev accuracy: 0.804\n",
            "Epoch 7, dev accuracy: 0.811\n",
            "Epoch 8, dev accuracy: 0.813\n",
            "Epoch 9, dev accuracy: 0.818\n",
            "Epoch 10, dev accuracy: 0.807\n",
            "Epoch 11, dev accuracy: 0.812\n",
            "Epoch 12, dev accuracy: 0.814\n",
            "Epoch 13, dev accuracy: 0.814\n",
            "Epoch 14, dev accuracy: 0.811\n",
            "\n",
            "Best Performing Model achieves dev accuracy of : 0.818\n"
          ]
        }
      ],
      "source": [
        "# Running this cell should take ~2 minutes.\n",
        "embs, cnn_vocab = read_embeddings(\"glove.6B.50d.50K.txt\")\n",
        "cnnmodel = CNNClassifier(params={\"max_seq_len\": 549, \"label_length\": len(labels)}, pretrained_embeddings=embs).to(device)\n",
        "optimizer = torch.optim.Adam(cnnmodel.parameters(), lr=0.001, weight_decay=1e-5)\n",
        "losses = []\n",
        "cross_entropy=nn.CrossEntropyLoss()\n",
        "\n",
        "num_epochs=15\n",
        "best_dev_acc = 0.\n",
        "patience=10\n",
        "\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    cnnmodel.train()\n",
        "\n",
        "    for x, y in zip(cnn_trainX, cnn_trainY):\n",
        "      x, y = x.to(device), y.to(device)\n",
        "      y_pred = cnnmodel.forward(x)\n",
        "      loss = cross_entropy(y_pred.view(-1, cnnmodel.num_labels), y.view(-1))\n",
        "      losses.append(loss) \n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "    dev_accuracy=cnnmodel.evaluate(cnn_devX, cnn_devY)\n",
        "   \n",
        "    # check if the dev accuracy is the best seen so far; save the model if so\n",
        "    print(\"Epoch %s, dev accuracy: %.3f\" % (epoch, dev_accuracy))\n",
        "    if dev_accuracy > best_dev_acc:\n",
        "      torch.save(cnnmodel.state_dict(), 'best-cnnmodel-parameters.pt')\n",
        "      best_dev_acc = dev_accuracy\n",
        "      patienceCounter=0\n",
        "        \n",
        "    patienceCounter+=1\n",
        "    if patienceCounter >= patience:\n",
        "        print(\"Stopping training; no improvement on dev data after %s epochs\" % patience)\n",
        "        break\n",
        "\n",
        "    \n",
        "cnnmodel.load_state_dict(torch.load('best-cnnmodel-parameters.pt'))\n",
        "print(\"\\nBest Performing Model achieves dev accuracy of : %.3f\" % (best_dev_acc))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8j5kM7T5T69d"
      },
      "source": [
        "# Model Exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7u2MerXT9fb"
      },
      "source": [
        "## CNN Loss Examination\n",
        "To debug your model and ensure it is updating correctly, it may be helpful to visualize your training loss.  The following code plots loss over epoch.  This should decrease as the model trains and eventually converge.  If your training loss is not decreasing, you might not be initializing your model or creating your forward() pass correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0W1WDYkfrdLX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "outputId": "3c0f4149-5501-493b-a0c7-e2e517f4403b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(range(len(losses)), losses)\n",
        "plt.title(\"Training Loss over Time\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAW6FujgeUOb"
      },
      "source": [
        "# Deliverable 3. BoW vs. CNN.\n",
        "\n",
        "Compare and contrast the performance of your BoW representation and CNN. Did one model demonstrate a higher dev performance than the other? What do you see as the advantages of one model over the other that might lead to this performance difference on this data? Submit your <200 word answer to this question as a PDF on gradescope."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It might come off as surprising that our BoW logistic regression model performs better than our CNN, given that we can consider logistic regression as a neural network without hidden layers. However, it only takes a closer look at our implementation that we realize the limitations that our first CNN has over the BoW.\n",
        "\n",
        "\n",
        "For instance, let's look at what our models \"look for\" when formulating our decision. For logistic regression, we have access to all the occurrences as well as non-occurrence of words per review allowing us to do fitting across all features. On the other hand, our CNN has n-grams and we limit ourselves to detecting patterns within ourselves, one, and two neighbors. This limited scope affects what a CNN will determine good and bad.\n",
        "\n",
        "\n",
        "While max pooling is that it provides a fixed size output matrix, (needed for classification), and gives out the most relevant information from our n-grams, other discerning features might've been left out, affecting the accuracy. On the other hand, BoW doesn't need to take the most relevant information, every word occurrence (and non-occurrence 0) are taken into consideration when performing logistic regression. "
      ],
      "metadata": {
        "id": "adGhkyBlG0tR"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Copy of HW_3.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}