{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JrBpOnJRe8xd"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dbamman/nlp22/blob/main/HW4/HW_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inWOGaHZMsUH"
      },
      "source": [
        "# Homework 4: Language Modeling\n",
        "\n",
        "In this homework, we will explore implementations of various language models we saw in lecture. We will use a dataset of movie reviews to learn statitics about words in our language to build our models. We will build classical N-Gram Models, measure perplexity, and generate text using the models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNBJieAJcdlY"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xJy9Ar2YFz6h",
        "outputId": "9ec89127-bda5-4da6-9bdf-eee1f9cc2622"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "import numpy as np\n",
        "import math\n",
        "import tqdm\n",
        "import random\n",
        "\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torchtext.legacy as torchtext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294,
          "referenced_widgets": [
            "d928e683e0274b93950a91c200111df5",
            "8a8a40bf9a534ba69a09d60d0230a8ec",
            "1bb9801de8f244739f0be67a5748535c",
            "7ebe9361bd8d43c594edaedabbeffea9",
            "05aab5ee516947df92b532b0ac0e209d",
            "e4f1c0d7c6c947cab896e7c90f25e2d0",
            "65501281d2e545778c7d5c49b6a5abea",
            "1a6717f3c86042af99f4afe6be8eaa35",
            "570ad696545a42beb8e0d4fd6c92c541",
            "fe11cbe39ef04bc9a02cb8bf28555194",
            "c453d8dbce8149b3b72a7f666c7c44f1"
          ]
        },
        "id": "EDW95kiEPWim",
        "outputId": "ef6c1788-1c03-4022-d4b0-7cd8735678e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-02-17 00:47:01--  https://raw.githubusercontent.com/dbamman/nlp22/main/HW4/plot_summaries.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 75934033 (72M) [text/plain]\n",
            "Saving to: ‘plot_summaries.txt.2’\n",
            "\n",
            "plot_summaries.txt. 100%[===================>]  72.42M   219MB/s    in 0.3s    \n",
            "\n",
            "2022-02-17 00:47:01 (219 MB/s) - ‘plot_summaries.txt.2’ saved [75934033/75934033]\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d928e683e0274b93950a91c200111df5",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/42303 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of words in the vocabulary: 57663\n",
            "Example text: ['he', 'stumbles', 'upon', 'a', 'jailbreak', 'and', 'knocks', 'out', 'the', 'convicts', '.', 'he', 'is', 'hailed', 'a', 'hero', 'and', 'is', 'released', '.', 'outside', 'the', 'jail', ',', 'he', 'discovers', 'life', 'is', 'harsh', ',']\n"
          ]
        }
      ],
      "source": [
        "# download and load the data\n",
        "!wget https://raw.githubusercontent.com/dbamman/nlp22/main/HW4/plot_summaries.txt\n",
        "data = pd.read_csv('plot_summaries.txt', sep='\\t', header=None)[1].tolist()\n",
        "\n",
        "def make_text(data):\n",
        "    all_text = []\n",
        "    for d in tqdm.notebook.tqdm(data):\n",
        "        all_text.append(\"<eos>\")\n",
        "        clean_text = \" \".join(d.lower().split())\n",
        "        all_text.extend(word_tokenize(clean_text))\n",
        "        all_text.append(\"<eos>\")\n",
        "    return all_text\n",
        "\n",
        "text = make_text(data)\n",
        "train_size, validation_size = 2000000, 200000\n",
        "train_text, validation_text = text[:train_size], text[train_size:train_size+validation_size]\n",
        "\n",
        "text_field = torchtext.data.Field()\n",
        "counter = Counter(train_text)\n",
        "text_field.vocab = text_field.vocab_cls(counter)\n",
        "vocab = text_field.vocab\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "print(\"Number of words in the vocabulary: {}\".format(vocab_size))\n",
        "print(\"Example text: {}\".format(validation_text[:30]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bYh1e5jYy5YL"
      },
      "outputs": [],
      "source": [
        "def make_dataset(text):\n",
        "    a = torchtext.data.Example.fromlist([\" \".join(text)], [('text', text_field)])\n",
        "    return torchtext.data.Dataset([a], [('text', text_field)])\n",
        "\n",
        "train_dataset, validation_dataset = make_dataset(train_text), make_dataset(validation_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SCcz1iY5Pd4v"
      },
      "outputs": [],
      "source": [
        "def ids(vocab, tokens):\n",
        "    \"\"\"Helper function to convert a list of words into indices in our vocab\"\"\"\n",
        "    return [vocab.stoi[t] for t in tokens]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AdUpgkFCe8xu",
        "outputId": "d5925bdc-2d88-4ffc-b36a-4a3392378716"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on cuda\n"
          ]
        }
      ],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Running on {}\".format(device))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xuyB3RfjPoGu"
      },
      "source": [
        "## Classical N-Gram Model\n",
        "\n",
        "For this part, we will build a classical N-Gram model to learn the statistics of our training data. To train this model, we simply count the number of times each $n$-gram occurs in our training text and divide it by the number of times the first $n-1$ words occur. Additionally, we will also add alpha-smoothing to make sure no word has $0$ probability. This is summed up in this equation:\n",
        "\n",
        "$$P(w_n|w_1 \\dots w_{n-1})=\\frac{C(w_1 \\dots w_n)+\\alpha}{C(w_1 \\dots w_{n-1})+\\alpha\\cdot|V|}$$\n",
        "\n",
        "where $|V|$ is the vocab size and $C$ is the count for the given n-gram.\n",
        "\n",
        "We will handle computing the counts for you and your job will be to simply fill in the functions to compute the above equation and the perplexity for the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0xJjoXN_F-s0"
      },
      "outputs": [],
      "source": [
        "class NGramModel:\n",
        "  ## For this part, we will build a classical N-Gram model to learn the statistics of our training data!\n",
        "  ##To TRAIN THIS MODEL we COUNT the number of times each n-gram occurs in our training, and DIVIDE by the number of times the first (n-1 words occur)\n",
        "  ##Apply alpha smoothing such that |V| is the vocab size, C isthe count for the given ngram\n",
        "    def __init__(self, train_text, vocab, n=2, alpha=3e-3):\n",
        "        # get counts and perform any other setup\n",
        "        self.n = n\n",
        "        self.smoothing = alpha\n",
        "        self.vocab = vocab\n",
        "\n",
        "        # count n-grams\n",
        "        self.counts_n = Counter()\n",
        "        curr = [\"<eos>\"] * (self.n - 1) # padding for initial words\n",
        "        for i in range(len(train_text)):\n",
        "            curr.append(train_text[i])\n",
        "            gram = \" \".join(curr)\n",
        "            self.counts_n[gram] += 1\n",
        "            curr.pop(0)\n",
        "\n",
        "        # count n-1-gram\n",
        "        self.counts_n_1 = Counter()\n",
        "        for c in self.counts_n:\n",
        "          temp = \" \".join(c.split(\" \")[:-1])\n",
        "          self.counts_n_1[temp] += self.counts_n[c]  \n",
        "\n",
        "    def get_probability(self, text):\n",
        "        \"\"\"Return the probability of the last word in an n-gram. This is the\n",
        "        equation given in the text above.\n",
        "        \n",
        "        Args:\n",
        "            text: a list of string tokens\n",
        "\n",
        "        Hints:\n",
        "            - self.counts_n contains the number of occurances for any string of n words\n",
        "            - self.count_n_1 contains the number of occurances for any string of n-1 words\n",
        "            - you can use the join method to create a string from a list of words\n",
        "            - Remember, the given string can have more than n words\n",
        "            - self.vocab contains a dictionary of the vocabulary\n",
        "        \"\"\"\n",
        "        assert len(text) >= self.n, f\"Expected at least {self.n} words; got {len(text)} words. \\nGiven text: {text}\"\n",
        "\n",
        "        # BEGIN SOLUTION\n",
        "        #print(\" \".join(text[-self.n:]))\n",
        "        #print(self.counts_n[\" \".join(text[-self.n:])])\n",
        "        #print((\" \".join(text[-self.n:-1])))\n",
        "        #print(self.counts_n_1[\" \".join(text[-self.n:-1])])\n",
        "        #print(\" \".join(text[-self.n:]))\n",
        "        #print(\" \".join(text[-(self.n - 1):]))\n",
        "        return (self.counts_n[\" \".join(text[-self.n:])] + self.smoothing) / (self.counts_n_1[\" \".join(text[-(self.n):-1])] + self.smoothing * len(self.vocab) )\n",
        "        # END SOLUTION\n",
        "\n",
        "    def get_next_word_probabilities(self, text_prefix):\n",
        "        \"\"\"Return a list of probabilities for each word in the vocabulary.\n",
        "\n",
        "        Args:\n",
        "            text_prefix: a list of string tokens\n",
        "\n",
        "        Hints:\n",
        "            - you need to use your get_probability function\n",
        "            - self.vocab.itos contains a list of words to return probabilities for\n",
        "            - you will need to handle the cases in which the text prefix is both\n",
        "                shorter and longer than n-1 words. For the shorter case, you need to\n",
        "                pad with \"<eos>\" tokens to the beginning of the text prefix. For the\n",
        "                longer case, you need to truncate the text prefix to the last n-1 words\n",
        "            - As a sanity check, you should make sure the probabilities you return add up to 1\n",
        "        \"\"\"\n",
        "        # BEGIN SOLUTION\n",
        "        #self.vocab.itos contains a list of words to return probabilities for:\n",
        "        t_p = text_prefix\n",
        "          #Case where text prefix < n-1 words\n",
        "        if(len(t_p) <= self.n - 1):\n",
        "          #Apply pad with \"<eos>\" tokens to the beginning of the text prefix\n",
        "          while( len(t_p) < self.n - 1):\n",
        "            t_p.insert(0,'<eos>')\n",
        "          #Case where text prefix > n-1 words:\n",
        "          #Truncate prefix to the last n-1 words\n",
        "        if(self.n == 1):\n",
        "          t_p = t_p[-1:]\n",
        "        else:\n",
        "          t_p = t_p[- (self.n - 1):]\n",
        "\n",
        "        probabilities = []\n",
        "        for word in self.vocab.itos:\n",
        "          probabilities += [self.get_probability(t_p + [word] )]\n",
        "        #print(sum(probabilities))\n",
        "        return probabilities\n",
        "        # END SOLUTION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pR5BV_IbFNbF"
      },
      "source": [
        "### Perplexity\n",
        "\n",
        "To evaluate how good our language model, we use a metric called perplexity. The perplexity of a language model (PP) on a test set is the inverse probability of the test set, normalized by the number of words. Let $W = w_{1}w_{2}\\dots w_{N}$. Then,\n",
        "\n",
        "$$PP(W) = \\sqrt[N]{\\prod_{i = 1}^{N}\\frac{1}{P(w_{i}|w_{1}\\dots w_{i - 1})}}$$\n",
        "\n",
        "However, since these probabilities are often small, taking the inverse and multiplying can be numerically unstable, so we often first compute these values in the log domain and then convert back. So this equation looks like:\n",
        "\n",
        "$$\\ln PP(W) = \\frac{1}{N} \\sum_{i = 1}^{N} -\\ln P(w_{i}|w_{1}\\dots w_{i - 1})$$\n",
        "\n",
        "$$\\implies PP(W) = e^{\\frac{1}{N} \\sum_{i = 1}^{N} -\\ln P(w_{i}|w_{1}\\dots w_{i - 1})}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qz99lOhKEPgT",
        "outputId": "6e3357a9-656c-4e6c-8035-d2af5c206e6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unigram validation perplexity: 1339.197981504648\n",
            "bigram validation perplexity: 805.0404106599412\n",
            "trigram validation perplexity: 5123.022246473123\n"
          ]
        }
      ],
      "source": [
        "def get_perplexity(model, text):\n",
        "    \"\"\"Returns the perplexity of the model.\n",
        "    \n",
        "    Args:\n",
        "        text: a list of string tokens\n",
        "\n",
        "    Hints:\n",
        "        - you need to use your model.get_probability function\n",
        "        - you need to handle the edge case for the first n-1 words of text. You\n",
        "            can similarly pad with \"<eos>\" tokens\n",
        "        - you want to get the probability of each increasing sequence of words\n",
        "    \"\"\"\n",
        "    # BEGIN SOLUTION\n",
        "    probabilities = []\n",
        "    for i in range(len(text)):\n",
        "      tempText = []\n",
        "      #lets say, ngram = 3 and we are at position 0, we get ourselves and two guys behind me [eos] [eos]\n",
        "      #Lets say, ngram = 3 and we are at position 1, we get ourselves and two guys behind me [word] [eos]\n",
        "      for n in range(model.n):\n",
        "        if(i - n < 0 ):\n",
        "          tempText += ['<eos>']\n",
        "        else:\n",
        "          tempText += [text[i-n]]\n",
        "      probabilities += [model.get_probability(tempText[::-1])]\n",
        "        \n",
        "    return math.e ** (np.mean( -np.log(probabilities)))\n",
        "\n",
        "\n",
        "    # END SOLUTION\n",
        "\n",
        "unigram_model = NGramModel(train_text, vocab, n=1)\n",
        "print('unigram validation perplexity:', get_perplexity(unigram_model, validation_text)) # should be around 1300-1400\n",
        "\n",
        "bigram_model = NGramModel(train_text, vocab, n=2)\n",
        "print('bigram validation perplexity:', get_perplexity(bigram_model, validation_text)) # should be around 800\n",
        "\n",
        "trigram_model = NGramModel(train_text, vocab, n=3)\n",
        "print('trigram validation perplexity:', get_perplexity(trigram_model, validation_text)) # this won't do very well (around 5000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79oQTkEMg-yG"
      },
      "source": [
        "### Deliverable 1\n",
        "\n",
        "Fill in the calculated perplexities given from the cell above and answer the question."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2i7Cgt1SgVbN"
      },
      "source": [
        "<!-- Do not remove this comment, it is used by the autograder: RqYJKsoTS6 -->\n",
        "\n",
        "Unigram validation perplexity: ***1339.198***\n",
        "\n",
        "Bigram validation perplexity: ***805.040***\n",
        "\n",
        "Trigram validation perplexity: ***5123.022***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23duN0Mkg7Zy"
      },
      "source": [
        "Question: Why does the trigram model have such a high perplexity?\n",
        "\n",
        "Answer: \n",
        "\n",
        "**Based on the chapter 3.4 of the texbook, we might be able to atrribute some of the complexity due to sparcity. where a bi-gram (eg. \"Denied the\") could ve detected more often than a tri-gram, (eg. \"Denied the passage\") due to the a limited corpus, hence giving really low probabilities or 0 (w/o) smoothing.**\n",
        "\n",
        "**On the other hand, it might be the fact that we have more word-to-word coherence within our corpus since not everything is by the same author (like 3-grams resembling Shakespear on the textbook), this could be because the summaries might've not been written by the same person, making the writting style different from summary to summary.**\n",
        "\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XBELAx_xcx4m"
      },
      "source": [
        "## Text Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGQqecOwcNfW"
      },
      "source": [
        "### Deliverable 2\n",
        "\n",
        "In this section, we will explore generating sentences using our models. Your job will be to simply fill-in the following function. You should try out the various models you have built and compare the types of sentences they generate. These models are of course very limited compared to current state of the art models that are able to model much longer sequences of text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tetguEyrHppc"
      },
      "outputs": [],
      "source": [
        "def generate_text(model, n=20, seed=0, prefix=['<eos>', '<eos>']):\n",
        "    \"\"\"Returns a randomly generated sentence sampled from the probability\n",
        "    distribution given by a language model.\n",
        "\n",
        "    Args:\n",
        "        model: language model\n",
        "        n: number of words to generate\n",
        "        prefix: list of tokens to prompt your model\n",
        "    \n",
        "    Hints:\n",
        "        - you need to use your model.get_next_word_probabilities function\n",
        "        - you can use the random.choices function to sample from a list according to probabilities\n",
        "        - model.vocab.itos contains a list of words in the vocabulary\n",
        "    \"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    # BEGIN SOLUTION\n",
        "    #Let's get a random word from our probabilities:\n",
        "    string = prefix\n",
        "    for i in range(n):\n",
        "      nextWord = random.choices(model.vocab.itos, model.get_next_word_probabilities( string ), k = 1)\n",
        "      #print(nextWord)\n",
        "      string += nextWord\n",
        "    return \" \".join(string)\n",
        "    # END SOLUTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UK0ZF-gfcUOS",
        "outputId": "21c40932-9021-4ae3-a769-6bc5fd10999f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<eos> <eos> portrayed minister it is finds who spy he have around suitable house in ralph attack of humanoid dragon-eye recent raping\n"
          ]
        }
      ],
      "source": [
        "unigram_string = generate_text(model = unigram_model, prefix=['<eos>', '<eos>'])\n",
        "print(unigram_string)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vs9Hq_y2E6ky",
        "outputId": "31aa3691-950c-4aa2-95c6-a7fb192fc786"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<eos> <eos> quiet anklet giff rajaguru comparisons blackburn introducers taller upper-middle-class sympathises saloons enroute youngster girlfriend/fiancée 8pm ransacked saburo wargames look–alike rocker\n"
          ]
        }
      ],
      "source": [
        "bigram_string = generate_text(model = bigram_model, prefix=['<eos>', '<eos>'])\n",
        "print(bigram_string)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "saHyG5dxE67w",
        "outputId": "bf72c201-f5d7-431c-9bbf-0233f8e760c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<eos> <eos> zed hammering hisako gala glues consigned jasmina 410 beauvoir undergarments santander famously emphasizes half-processed abstinance vocalist scalograph warnie loxahatchee romania.der\n"
          ]
        }
      ],
      "source": [
        "trigram_string = generate_text(model = trigram_model, prefix=['<eos>', '<eos>'])\n",
        "print(trigram_string)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dk5HfTLlCVBI"
      },
      "source": [
        "## Optional: Neural Models\n",
        "\n",
        "As you can see from the text you generated in the previous section, the capabilities of classical language models is quite limited. They simply learn the meanings of words in terms of counts and are limited to a fixed window of words. In this section, we will explore neural methods for language modeling, which are a bit closer to what modern language models look like.\n",
        "\n",
        "You don't have to do anything in section and there is no deliverable. It is merely here for you to explore how neural language models work. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFw-oZhXcUId"
      },
      "source": [
        "### Neural N-Gram Model\n",
        "\n",
        "Now, we will train a neural network to model our language. We will use a feedforward network that takes in the previous $n-1$ words and outputs a distribution over the vocabulary which can be used to form a probability of the next word.\n",
        "\n",
        "We will implement this model using PyTorch and its various utilities for dataloading."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34,
          "referenced_widgets": [
            "4b64df5493924678a9457c6634994bf6",
            "c5139cd08f074ba0ba6ed10c1d0516ec",
            "4e14f7b7aa6145e1a3522e6b3685e286",
            "187eba35631a45d68decc9b27aa9737f",
            "7b8a98f31add4c2a9fc4d9ce11896935",
            "b758b73fa28b4075bcce82be6a5b9737",
            "4d61842f323f4039bb26ac302833da7d",
            "83ef0c40dfc94bf1b3c3cbff707f015f",
            "0f6f5f149a074a58bddf98a885c8be3f",
            "246085b9da724b9798a8cbe2e9d63ecb",
            "a0eef80dba724c9f86f43c2c6880f6e3"
          ]
        },
        "id": "WuBVYDScI3w1",
        "outputId": "27354ff5-be73-42d2-f78e-50e7ec939334"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4b64df5493924678a9457c6634994bf6",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/15625 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "class NgramDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, text_token_ids, n):\n",
        "        self.text_token_ids = text_token_ids\n",
        "        self.n = n\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text_token_ids)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        if i < self.n-1:\n",
        "            prev_token_ids = [vocab.stoi['<eos>']] * (self.n-i-1) + self.text_token_ids[:i]\n",
        "        else:\n",
        "            prev_token_ids = self.text_token_ids[i-self.n+1:i]\n",
        "\n",
        "        x = torch.tensor(prev_token_ids)\n",
        "        y = torch.tensor(self.text_token_ids[i])\n",
        "        return x, y\n",
        "\n",
        "class NeuralNGram(nn.Module):\n",
        "    def __init__(self, n):\n",
        "        super().__init__()\n",
        "        self.n = n\n",
        "\n",
        "        self.linear_1 = nn.Linear((self.n - 1) * 128, 256)\n",
        "        self.linear_2 = nn.Linear(256, 128)\n",
        "        self.dropout = nn.Dropout(p=0.1)\n",
        "        self.linear_3 = nn.Linear(128, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Returns a tensor of log probabilities with shape (batch, vocab_size).\n",
        "\n",
        "        Args:\n",
        "            x: tensor of input with shape (batch, n-1)\n",
        "        \"\"\"\n",
        "        x = F.embedding(x, weight=self.linear_3.weight) # weight tying\n",
        "        x = x.reshape(x.size(0), -1)\n",
        "        x = self.linear_1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.linear_2(x)\n",
        "        x = self.linear_3(x)\n",
        "        return x\n",
        "\n",
        "class NeuralNGramModel:\n",
        "    def __init__(self, n, vocab):\n",
        "        self.n = n\n",
        "        self.vocab = vocab\n",
        "        self.network = NeuralNGram(n).to(device)\n",
        "\n",
        "    def train(self):\n",
        "        dataset = NgramDataset(ids(self.vocab, train_text), self.n)\n",
        "        train_loader = torch.utils.data.DataLoader(dataset, batch_size=128, shuffle=True)\n",
        "        optim = torch.optim.Adam(self.network.parameters())\n",
        "        prev_validation = float(\"inf\")\n",
        "\n",
        "        for epoch in range(3):\n",
        "            print(\"Epoch\", epoch)\n",
        "            self.network.train()\n",
        "\n",
        "            for prev, curr in tqdm.notebook.tqdm(train_loader, leave=False):\n",
        "                prev, curr = prev.to(device), curr.to(device)\n",
        "                optim.zero_grad()\n",
        "                output = self.network(prev)\n",
        "                loss = F.cross_entropy(output, curr)\n",
        "                loss.backward()\n",
        "                optim.step()\n",
        "\n",
        "            # save the model with the best validation perplexity\n",
        "            validation_pp = get_perplexity(self, validation_text)\n",
        "            print(\"Validation score:\", validation_pp)\n",
        "\n",
        "            if validation_pp < prev_validation:\n",
        "                torch.save(self.network.state_dict(), \"neural_language_model.pkl\")\n",
        "                prev_validation = validation_pp\n",
        "        \n",
        "        # load best saved model\n",
        "        self.network.load_state_dict(torch.load(\"neural_language_model.pkl\"))\n",
        "    \n",
        "    def get_probability(self, text):\n",
        "        assert len(text) >= self.n, f\"Expected at least {self.n} words; got {len(text)} words. \\nGiven text: {text}\"\n",
        "        target_id = ids(self.vocab, [text[-1]])[0]\n",
        "        return self.get_next_word_probabilities(text[:-1])[target_id]\n",
        "\n",
        "    def get_next_word_probabilities(self, text_prefix):\n",
        "        self.network.eval()\n",
        "        while len(text_prefix) < self.n - 1:\n",
        "            text_prefix = [\"<eos>\"] + text_prefix\n",
        "        if len(text_prefix) > self.n - 1:\n",
        "            text_prefix = text_prefix[len(text_prefix) - self.n + 1 :]\n",
        "        x = torch.Tensor(ids(self.vocab, text_prefix)).to(torch.int64).to(device)\n",
        "        x = x.reshape((1, len(x)))\n",
        "        with torch.no_grad():\n",
        "            probs = F.softmax(self.network(x), dim=1)[0]\n",
        "        return probs.cpu()\n",
        "\n",
        "neural_trigram_model = NeuralNGramModel(3, vocab)\n",
        "neural_trigram_model.train()\n",
        "print('neural trigram validation perplexity:', get_perplexity(neural_trigram_model, validation_text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8jFjmrTcFIJ"
      },
      "source": [
        "### RNN Model\n",
        "\n",
        "Now, we will train a Recurrnt Neural Network to model our language. This is a much more flexible architecture for language modeling since it is able to handle inputs of any length and can thus model longer ranges of text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y4W-4ExjdFCS"
      },
      "outputs": [],
      "source": [
        "num_hidden_rnn_layers = 1\n",
        "class RNNNetwork(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.rnn = nn.RNN(128, 128, num_hidden_rnn_layers, dropout=0.5).to(device)\n",
        "        self.linear = nn.Linear(128, 128).to(device)\n",
        "        self.linear_2 = nn.Linear(128, vocab_size).to(device)\n",
        "        self.dp = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x, state):\n",
        "        x = F.embedding(x, weight=self.linear_2.weight) # weight tying\n",
        "        x, state = self.rnn(x, state)\n",
        "        x = self.dp(x)\n",
        "        x = self.linear(x)\n",
        "        x = self.linear_2(x)\n",
        "        return x, state\n",
        "\n",
        "class RNNModel:\n",
        "\n",
        "    def __init__(self, vocab):\n",
        "        self.n = 2 # makes it compatible with other n-gram models\n",
        "        self.vocab = vocab\n",
        "        self.network = RNNNetwork().to(device)\n",
        "\n",
        "    def train(self):\n",
        "        train_iterator = torchtext.data.BPTTIterator(train_dataset, batch_size=64, \n",
        "                                                     bptt_len=32, device=device)\n",
        "  \n",
        "        h = torch.autograd.Variable(torch.zeros(num_hidden_rnn_layers, 64, self.network.rnn.hidden_size), requires_grad=False).to(device)\n",
        "        optim = torch.optim.Adam(self.network.parameters())\n",
        "        prev_validation = float('inf')\n",
        "        for epoch in range(20):\n",
        "          print('Epoch', epoch + 1)\n",
        "          self.network.train()\n",
        "          for batch in tqdm.notebook.tqdm(train_iterator, leave=False):\n",
        "            assert self.network.training, 'make sure your network is in train mode with `.train()`'\n",
        "            text, target = batch.text, batch.target\n",
        "            text, target = text.to(torch.int64).to(device), target.to(torch.int64).to(device)\n",
        "            optim.zero_grad()\n",
        "\n",
        "            output, h = self.network(text, h)\n",
        "            output = output.view(-1, output.shape[-1])\n",
        "            target = target.view(-1,)\n",
        "            loss = F.cross_entropy(output, target)\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "            h = h.detach()\n",
        "\n",
        "          validation_pp = get_perplexity(self, validation_text)\n",
        "          print('Validation score:', validation_pp)\n",
        "\n",
        "          if validation_pp < prev_validation:\n",
        "            torch.save(self.network.state_dict(), \"rnn_language_model.pkl\")\n",
        "            prev_validation = validation_pp\n",
        "\n",
        "        self.network.load_state_dict(torch.load(\"rnn_language_model.pkl\"))\n",
        "    \n",
        "    def get_probability(self, text):\n",
        "        target_id = ids(self.vocab, [text[-1]])[0]\n",
        "        return self.get_next_word_probabilities(text[-32:-1])[target_id]\n",
        "\n",
        "    def get_next_word_probabilities(self, text_prefix):\n",
        "        prefix_token_tensor = torch.tensor(ids(self.vocab, text_prefix), device=device).view(-1, 1)\n",
        "        prefix_token_tensor = prefix_token_tensor.to(torch.int64).to(device)\n",
        "        h = torch.autograd.Variable(next(self.network.parameters()).data.new(num_hidden_rnn_layers, 1, self.network.rnn.hidden_size), requires_grad=False)\n",
        "        self.network.eval()\n",
        "        with torch.no_grad():\n",
        "          output, _ = self.network(prefix_token_tensor, h)\n",
        "          output = output.squeeze(dim=1)\n",
        "          probs = F.softmax(output, dim=-1)\n",
        "        return probs[-1]\n",
        "\n",
        "rnn_model = RNNModel(vocab)\n",
        "rnn_model.train()\n",
        "print('rnn validation perplexity:', get_perplexity(rnn_model, validation_text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6So9wKWlhIyl"
      },
      "source": [
        "## Submission\n",
        "\n",
        "Congrats on making it to the end of the notebook. Please download this notebook and upload it to gradescope. Make sure all of the cells where your answers are expected are filled in."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Copy of HW_4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d928e683e0274b93950a91c200111df5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_8a8a40bf9a534ba69a09d60d0230a8ec",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_1bb9801de8f244739f0be67a5748535c",
              "IPY_MODEL_7ebe9361bd8d43c594edaedabbeffea9",
              "IPY_MODEL_05aab5ee516947df92b532b0ac0e209d"
            ]
          }
        },
        "8a8a40bf9a534ba69a09d60d0230a8ec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1bb9801de8f244739f0be67a5748535c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_e4f1c0d7c6c947cab896e7c90f25e2d0",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_65501281d2e545778c7d5c49b6a5abea"
          }
        },
        "7ebe9361bd8d43c594edaedabbeffea9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_1a6717f3c86042af99f4afe6be8eaa35",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 42303,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 42303,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_570ad696545a42beb8e0d4fd6c92c541"
          }
        },
        "05aab5ee516947df92b532b0ac0e209d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_fe11cbe39ef04bc9a02cb8bf28555194",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 42303/42303 [01:51&lt;00:00, 366.34it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c453d8dbce8149b3b72a7f666c7c44f1"
          }
        },
        "e4f1c0d7c6c947cab896e7c90f25e2d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "65501281d2e545778c7d5c49b6a5abea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1a6717f3c86042af99f4afe6be8eaa35": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "570ad696545a42beb8e0d4fd6c92c541": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "fe11cbe39ef04bc9a02cb8bf28555194": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c453d8dbce8149b3b72a7f666c7c44f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4b64df5493924678a9457c6634994bf6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_c5139cd08f074ba0ba6ed10c1d0516ec",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_4e14f7b7aa6145e1a3522e6b3685e286",
              "IPY_MODEL_187eba35631a45d68decc9b27aa9737f",
              "IPY_MODEL_7b8a98f31add4c2a9fc4d9ce11896935"
            ]
          }
        },
        "c5139cd08f074ba0ba6ed10c1d0516ec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4e14f7b7aa6145e1a3522e6b3685e286": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_b758b73fa28b4075bcce82be6a5b9737",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4d61842f323f4039bb26ac302833da7d"
          }
        },
        "187eba35631a45d68decc9b27aa9737f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_83ef0c40dfc94bf1b3c3cbff707f015f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 15625,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 15625,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0f6f5f149a074a58bddf98a885c8be3f"
          }
        },
        "7b8a98f31add4c2a9fc4d9ce11896935": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_246085b9da724b9798a8cbe2e9d63ecb",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 15622/15625 [03:35&lt;00:00, 72.84it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a0eef80dba724c9f86f43c2c6880f6e3"
          }
        },
        "b758b73fa28b4075bcce82be6a5b9737": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4d61842f323f4039bb26ac302833da7d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "83ef0c40dfc94bf1b3c3cbff707f015f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0f6f5f149a074a58bddf98a885c8be3f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "246085b9da724b9798a8cbe2e9d63ecb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a0eef80dba724c9f86f43c2c6880f6e3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}